# -*- coding: utf-8 -*-
"""
DAS ä¸¤ä½ç½®ä¿¡å·ç›¸ä¼¼åº¦è¯„ä¼°ï¼ˆæ•´æ´ç‰ˆï¼‰
- å³°æ£€æµ‹ä¸æˆªå–
- é¢‘è°±å¯è§†åŒ–ä¸è°æ³¢æå–
- RP å›¾ç›¸ä¼¼åº¦ï¼ˆSSIM/NCC/ç›´æ–¹å›¾äº¤å¹¶ï¼‰
- é¢‘è°±ç›¸ä¼¼åº¦ï¼ˆWelch  cos_psd/coherence/xcorrï¼‰
- POC  ç›¸ä½æ–œç‡å»¶æ—¶ï¼ˆbatchï¼‰
- ç»Ÿä¸€ä¿å­˜ CSVï¼ˆæ¯ä¸ª .npy ä¸€ä¸ªç»“æœè¡¨ï¼‰
"""

#  ------------------------------------------------------------------------------
#  Copyright (c) 2025 Chaos
#  All rights reserved.
#  #
#  This software is proprietary and confidential.
#  Licensed exclusively to Shineway Technologies, Inc for internal use only,
#  according to the NDA / agreement signed on 2025.11.26
#  Unauthorized redistribution or disclosure is prohibited.
#  ------------------------------------------------------------------------------
#
#

import os
import time
from typing import List, Dict, Iterable, Optional, Set
import datetime
import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
from scipy.signal import find_peaks
from numpy.typing import ArrayLike
from pathlib import Path
from typing import Optional

# ===== ä½ è‡ªå·±çš„å·¥å…·åŒ… =====
from utils.create_RP import create_RP
from utils.search_peaks import detect_peaks_1d, extract_peak_columns
from utils.image_match_v3 import compute_image_similarity_RP
from utils.frequency_similarity import combined_signal_similarity  # ä½ ä¹‹å‰çš„é¢‘è°±ç›¸ä¼¼åº¦å‡½æ•°
from utils.POC import SpecSimParams, batch_events_similarity
from utils.bin_to_npy import convert_bin_dir

# ======================
# å…¨å±€é…ç½®
# ======================
LIST_DIR = "../bjd9.24/bin"   # å­˜æ”¾ .npy çš„ç›®å½•
OUT_DIR  = "outputs"             # è¾“å‡ºç›®å½•ï¼ˆå›¾ç‰‡ä¸ csvï¼‰
os.makedirs(OUT_DIR, exist_ok=True)

# ä¿¡å·/åˆ†æå‚æ•°ï¼ˆè¯·æŒ‰å®é™…æ”¹ï¼‰
FS_SIGNAL      = 200.0   # ä¿¡å·é‡‡æ ·ç‡ï¼ˆHzï¼‰â€”â€”ç”¨äº FFT/é¢‘è°±ç›¸ä¼¼åº¦
ANALYSIS_BAND  = (0.2, 40.0)   # é¢‘è°±ç›¸ä¼¼åº¦åˆ†æé¢‘å¸¦
WELCH_NPERSEG  = 32      # é¢‘è°±ç›¸ä¼¼åº¦é‡Œç”¨çš„ Welch å‚æ•°
WELCH_NOVERLAP = 16

# POC/ç›¸ä½æ–œç‡åˆ†æå‚æ•°
POC_FS         = 200.0   # POC/ç›¸ä½æ–œç‡åˆ†æé‡‡ç”¨çš„é‡‡æ ·ç‡ï¼ˆå»ºè®®ä¸ FS_SIGNAL ä¿æŒä¸€è‡´ï¼‰
POC_PARAMS = SpecSimParams(
    fs=POC_FS,
    nperseg=64,          # çŸ­çª—å»ºè®® 64/32
    noverlap=32,
    fmin=1.0,
    fmax=min(40.0, POC_FS/2 - 1e-6),
    use_log_psd=True,
    coh_stat="median",
    freq_scale_search=False,
    s_range=(0.95, 1.05),
    s_steps=31
)

# å³°æ£€æµ‹/æˆªå–
SMOOTH_WINDOW = 9
SMOOTH_POLY   = 2
PROMINENCE    = 0.7
MIN_DISTANCE  = 120

# FFT å¯è§†åŒ–
FFT_ZPF       = 8      # é›¶å¡«å……å€æ•°
FFT_FMAX      = 60.0   # ç”»å›¾é¢‘ç‡ä¸Šé™ï¼ˆHzï¼‰
PEAK_HEIGHT   = 0.05   # é¢‘è°±å³°æ£€æµ‹é˜ˆå€¼ï¼ˆæŒ‰å¹…åº¦ï¼‰

# ======================
# å·¥å…·å‡½æ•°
# ======================

# ======================
# æ ¸å¿ƒå¤„ç†å•ä¸ªæ–‡ä»¶
# å¯é…ç½®ï¼šç›®å½•çº§è¾“å‡ºå­ç›®å½•å
# ======================

OUTPUT_SUBDIR_NAME = "_out"  # ç»Ÿä¸€æ”¾åœ¨â€œæ ¹ç›®å½•/_outâ€ä¸‹é¢


def _ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def _global_out_root() -> Path:
    """
    ç¡®å®š _out æ ¹ç›®å½•ï¼š
      1. ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ OUT_ROOTï¼ˆæ¨èåœ¨ Docker é‡Œæ˜¾å¼è®¾ç½®ï¼‰
      2. å…¶æ¬¡ä½¿ç”¨ IO_BASE_DIR / BASE_DIRï¼ˆå’Œ watcher çš„ base_dir ä¸€è‡´ï¼‰
      3. æœ€åé€€å›å½“å‰å·¥ä½œç›®å½•
    """
    root = os.getenv("OUT_ROOT")
    if root:
        base = Path(root)
    else:
        base_dir_env = os.getenv("IO_BASE_DIR") or os.getenv("BASE_DIR")
        if base_dir_env:
            base = Path(base_dir_env)
        else:
            base = Path.cwd()
    base = base.resolve()
    out_root = base / OUTPUT_SUBDIR_NAME
    _ensure_dir(out_root)
    return out_root

def _single_file_outdir(base: str) -> Path:
    """
    å•ç›®å½•æ¨¡å¼ï¼š
      ${OUT_ROOT}/_out/single/<file1>/
    """
    root = _global_out_root()
    d = root / "single" / base
    _ensure_dir(d)
    return d

def _double_file_outdir(pair_base: str) -> Path:
    """
    åŒç›®å½•æ¨¡å¼ï¼š
      ${OUT_ROOT}/_out/double/<file1>_VS_<file2>/
    """
    root = _global_out_root()
    d = root / "double" / pair_base
    _ensure_dir(d)
    return d

# ä¸ºäº†å…¼å®¹è€ä»£ç ï¼Œè¿™é‡Œå®šä¹‰ä¸€ä¸ªâ€œæ–°çš„â€ _new_run_outdir è¦†ç›–å‰é¢çš„ç‰ˆæœ¬
def _new_run_outdir(dirpath: str, category: str) -> Path:
    """
    ç›®å½•çº§ï¼ˆæ¯”å¦‚ç›®å½•æ–¹å·®å›¾å’Œ hourly æ±‡æ€»è¡¨ï¼‰çš„è¾“å‡ºç›®å½•ï¼š
      ${OUT_ROOT}/_out/<category>/<hourName>_<timestamp>/
    """
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    hour_name = Path(dirpath).name  # å°æ—¶ç›®å½•åï¼Œä¾‹å¦‚ 2025-11-08_13
    root = _global_out_root()
    outdir = root / category / f"{hour_name}_{ts}"
    _ensure_dir(outdir)
    return outdir

##########################

##########################
def safe_plot_spectrum(sig: ArrayLike, fs: float, title: str, out_path: str | None = None):
    """çª—å£åŒ–é›¶å¡«å……çš„ä½é¢‘è°±å›¾ï¼ˆå¯é€‰ä¿å­˜ï¼‰"""
    x = np.asarray(sig, dtype=np.float64)
    N = len(x)
    win = np.hanning(N)
    xw  = (x - x.mean()) * win
    X   = np.fft.rfft(xw, n=N * FFT_ZPF)
    freq = np.fft.rfftfreq(N * FFT_ZPF, d=1.0/fs)
    amp  = (2.0 / (win.sum() / N)) * np.abs(X) / N

    mask = (freq >= 0) & (freq <= FFT_FMAX)
    plt.figure(figsize=(7, 4))
    plt.plot(freq[mask], amp[mask])
    plt.xlim(0, FFT_FMAX)
    plt.xlabel("Frequency (Hz)")
    plt.ylabel("Amplitude")
    plt.grid(True, alpha=0.3)
    plt.title(title)
    if out_path:
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close()
    return freq, amp


def find_harmonics(freq: np.ndarray, amp: np.ndarray, height: float = PEAK_HEIGHT):
    """åœ¨è°±ä¸Šæ‰¾å³°â€”â€”æ³¨æ„è¿™é‡Œå¯¹çš„æ˜¯ ampï¼Œä¸æ˜¯åŸå§‹æ—¶åŸŸ x"""
    peaks, props = find_peaks(amp, height=height)
    return freq[peaks], amp[peaks]


def pack_file_result(base: str,
                     rp_final: float,
                     rp_ssim: float,
                     rp_ncc: float,
                     rp_hist: float,
                     freq_score: float,
                     parts: dict,
                     poc_df_row: pd.Series | None) -> pd.DataFrame:
    """æŠŠæ‰€æœ‰æŒ‡æ ‡æ‰“åŒ…æˆä¸€è¡Œ DataFrameï¼Œä¾¿äºæŒ‰æ–‡ä»¶æ±‡æ€»"""
    row = {
        "file": base,
        "rp_final(%)": rp_final,
        "rp_ssim": rp_ssim,
        "rp_ncc": rp_ncc,
        "rp_hist": rp_hist,
        "freq_score(%)": freq_score,
        "cos_psd_simple": parts.get("cos_psd", np.nan),
        "coherence_simple": parts.get("coherence", np.nan),
        "xcorr_simple": parts.get("xcorr", np.nan),
    }
    if poc_df_row is not None and isinstance(poc_df_row, pd.Series):
        row.update({
            "cos_psd": poc_df_row.get("cos_psd", np.nan),
            "coh_med": poc_df_row.get("coh_med", np.nan),
            "poc_peak": poc_df_row.get("poc_peak", np.nan),
            "delay_phase_ms": poc_df_row.get("delay_phase_ms", np.nan),
            "delay_poc_ms": poc_df_row.get("delay_poc_ms", np.nan),
            "delay_diff_ms": poc_df_row.get("delay_diff_ms", np.nan),
            "phase_delay_r2": poc_df_row.get("phase_delay_r2", np.nan),
        })
    return pd.DataFrame([row])


# ======================
# æ ¸å¿ƒå¤„ç†å•ä¸ªæ–‡ä»¶
# å¯é…ç½®ï¼šç›®å½•çº§è¾“å‡ºå­ç›®å½•å
# ======================

OUTPUT_SUBDIR_NAME = "_out"  # æ¯ä¸ªæ•°æ®ç›®å½•ä¸‹ä¼šç”Ÿæˆè¿™ä¸ªå­ç›®å½•æ¥æ”¾æ‰€æœ‰è¾“å‡º

def _dir_outdir(dirpath: str, subdir_name: str = OUTPUT_SUBDIR_NAME) -> str:
    """
    ç»™å®šæŸç›®å½•ï¼Œè¿”å›è¯¥ç›®å½•çš„ç›®å½•çº§è¾“å‡ºç›®å½•ï¼š
    <dirpath>/<subdir_name>/
    """
    outdir = os.path.join(os.path.abspath(dirpath), subdir_name)
    os.makedirs(outdir, exist_ok=True)
    return outdir


def _ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def _file_outdir(path: str, subdir_name: str = OUTPUT_SUBDIR_NAME) -> Path:
    """
    ç»™å®šâ€œåŸå§‹æ–‡ä»¶è·¯å¾„â€æˆ–â€œä½ å¸Œæœ›çš„åŸºå‡†è·¯å¾„â€ï¼Œç”Ÿæˆè¯¥æ–‡ä»¶çš„è¾“å‡ºç›®å½•ï¼š
    <parent>/<subdir_name>/<file_stem>/
    """
    p = Path(path).resolve()
    base = p.stem
    outdir = p.parent / subdir_name / base
    _ensure_dir(outdir)
    return outdir

def process_single_array(arr2d: np.ndarray, path: str, max_rows: Optional[int] = None) -> pd.DataFrame:
    """
    - arr2d: è¯»è‡ª .bin çš„ 2D æ•°ç»„ï¼ˆrows x blocksï¼‰æˆ– 1D åºåˆ—
    - path : åŸå§‹æ–‡ä»¶çš„ç»å¯¹/ç›¸å¯¹è·¯å¾„ï¼›å†³å®šè¾“å‡ºç›®å½•åå’Œ base
    """

    p = Path(path).resolve()
    base = p.stem
    outdir = _single_file_outdir(base)     # <dir>/_out/<base>/
    peaks_dir = outdir / "peaks"                        # å³°ç›¸å…³ä¸­é—´ä»¶
    spec_dir  = outdir / "spectrum"                     # é¢‘è°±å›¾ä¸è°æ³¢è¡¨
    rp_dir    = outdir / "rp"                           # RP å›¾äº§ç‰©ï¼ˆcreate_RP ä¼šç”¨åˆ°ï¼‰
    _ensure_dir(peaks_dir); _ensure_dir(spec_dir); _ensure_dir(rp_dir)

    # 1) é€‰æ‹© 1D åºåˆ—ï¼šä¸ä¹‹å‰é€»è¾‘ä¸€è‡´ï¼ŒäºŒç»´å–ç¬¬ 0 è¡Œ
    y = arr2d.astype(np.float32, copy=False)
    if y.ndim == 2:
        y = y[5, :]

    y = np.asarray(y, dtype=np.float32).ravel()
    if max_rows is not None and max_rows > 0 and len(y) > max_rows:
        y = y[:max_rows]

    # 2) å³°æ£€æµ‹
    #todo æœ¬å¥—æ•°æ®å¼ºåº¦éƒ½ä¸ºè´Ÿå€¼ï¼Œæ‰€ä»¥æˆ‘åœ¨ä¸¤ä¸ªutilså†…éƒ¨å¢åŠ äº†å–è´Ÿ
    df_peaks = detect_peaks_1d(
        y,
        prominence=PROMINENCE,
        distance=MIN_DISTANCE,
        smooth_window=SMOOTH_WINDOW,
        smooth_poly=SMOOTH_POLY
    )
    (peaks_dir / f"{base}_peaks.csv").write_text(
        df_peaks.to_csv(index=False, encoding="utf-8-sig"),
        encoding="utf-8-sig"
    ) if not df_peaks.empty else None

    if df_peaks.empty:
        # æ— å³°ï¼šä»è¿”å›ä¸€è¡Œç©ºæŒ‡æ ‡ï¼Œæ–¹ä¾¿æ±‡æ€»
        return pd.DataFrame([{"file": base}])

    # 3) æå–ä¸¤ä¸ªå³°çš„å±€éƒ¨æ³¢å½¢åˆ—ï¼ˆy_peaks: shape=(L, 2)ï¼‰
    y_peaks, x_peaks, idx_df = extract_peak_columns(
        y, df_peaks, show=False,
        outdir=str(peaks_dir)     # ä¼ ç»™ä½ åŸå‡½æ•°ï¼Œä½¿ç”¨å·²è§„èŒƒçš„ç›®å½•
    )

    # å¯é€‰ï¼šæŠŠå³°æ®µä¿å­˜ä¸‹æ¥ï¼Œä¾¿äºå¤ç°ï¼ˆé¿å…åå¤è®¡ç®—ï¼‰
    # np.save(str(peaks_dir / f"{base}_y_peaks.npy"), y_peaks)
    # np.save(str(peaks_dir / f"{base}_x_peaks.npy"), x_peaks)
    idx_df.to_csv(peaks_dir / f"{base}_peaks_idx.csv", index=False, encoding="utf-8-sig")

    # 4) é¢‘è°±å¯è§†åŒ–  è°æ³¢åˆ—è¡¨ï¼ˆå¯¹ä¸¤åˆ—åˆ†åˆ«ç”»ï¼‰
    for col in (0, 1):
        freq, amp = safe_plot_spectrum(
            y_peaks[:, col], FS_SIGNAL,
            title=f"{base} - PeakCol{col} Spectrum",
            out_path=str(spec_dir / f"{base}_spec_col{col}.png")
        )
        h_freqs, h_amps = find_harmonics(freq, amp, height=PEAK_HEIGHT)
        pd.DataFrame({"harmonic_freq": h_freqs, "harmonic_amp": h_amps}) \
          .to_csv(spec_dir / f"{base}_harmonics_col{col}.csv", index=False, encoding="utf-8-sig")

    # 5) RP å›¾  å›¾åƒç›¸ä¼¼åº¦ï¼ˆæŠŠ OUTDIR æŒ‡å‘ rp_dirï¼Œé¿å…å’Œåˆ«çš„è¾“å‡ºæ··æ”¾ï¼‰
    RP_images = create_RP(y_peaks, eps=0.005, steps=255, OUTDIR=str(rp_dir))
    rp_final, rp_details = compute_image_similarity_RP(
        RP_images[0], RP_images[1], return_details=True
    )
    rp_ssim = rp_details["ssim"]
    rp_ncc  = rp_details["ncc"]
    rp_hist = rp_details["hist"]

    # 6) é¢‘è°±ç›¸ä¼¼åº¦
    freq_score, parts = combined_signal_similarity(
        y_peaks[:, 0], y_peaks[:, 1],
        fs=FS_SIGNAL, nperseg=WELCH_NPERSEG, noverlap=WELCH_NOVERLAP,
        fmin=ANALYSIS_BAND[0], fmax=ANALYSIS_BAND[1]
    )
    seg_a = y_peaks[:, 0]
    seg_b = y_peaks[:, 1]

    # 7) POC  ç›¸ä½æ–œç‡ï¼ˆå•äº‹ä»¶ï¼šæ•´æ®µï¼‰
    events = [(0, len(y_peaks))]
    poc_df = batch_events_similarity(seg_a, seg_b, events, POC_PARAMS, None)
    poc_df["freq_score"]       = freq_score
    poc_df["cos_psd_simple"]   = parts["cos_psd"]
    poc_df["coherence_simple"] = parts["coherence"]
    poc_df["xcorr_simple"]     = parts["xcorr"]

    # ä¿å­˜å•æ–‡ä»¶è¯¦ç»†ç»“æœï¼ˆäº‹ä»¶çº§ï¼‰åˆ° <dir>/_out/<base>/similarity/
    sim_dir = outdir / "similarity"
    _ensure_dir(sim_dir)
    poc_df.to_csv(sim_dir / f"{base}_similarity_detail.csv", index=False, encoding="utf-8-sig")

    # 8) æ±‡æ€»è¡Œï¼ˆè¿”å›ç»™ä¸Šå±‚åšç›®å½•çº§/å…¨å±€æ±‡æ€»ï¼‰
    file_df = pack_file_result(
        base=base,
        rp_final=rp_final, rp_ssim=rp_ssim, rp_ncc=rp_ncc, rp_hist=rp_hist,
        freq_score=freq_score, parts=parts,
        poc_df_row=poc_df.iloc[0] if not poc_df.empty else None
    )
    return file_df



# ======================
# æ–°å¢ï¼šä»ä¸¤ä¸ªä¸åŒè¾“å…¥ç›®å½•å„å–â€œæœ€å¤§ prominence å³°â€åè¿›è¡Œæ¯”è¾ƒ
# ======================
def _pick_top_prom_peak_segment(y: np.ndarray, df_peaks: pd.DataFrame) -> np.ndarray:
    """
    åœ¨ df_peaks ä¸­æŒ‰ prominence æœ€å¤§æŒ‘å‡ºä¸€ä¸ªå³°ï¼Œè¿”å›è¯¥å³°çš„æ—¶åŸŸç‰‡æ®µ (1D)ã€‚
    æ³¨ï¼šextract_peak_columns å·²ç»æŠŠ left/right åšäº†æ•´æ•°åŒ–ä¸è¾¹ç•Œæ ¡æ­£ã€‚
    """
    if df_peaks.empty:
        raise ValueError("No peaks found.")
    # å–æœ€å¤§ prominence çš„è¡Œ
    row = df_peaks.sort_values("prominence", ascending=False).iloc[0]
    # æ„é€ åªå«è¯¥å³°çš„ DataFrame ä»¥å¤ç”¨ extract_peak_columns
    df_one = pd.DataFrame([row])[["peak_id","left_ip","right_ip","peak_index","prominence","height","width_samples"]]
    y_cols, _, idx_df = extract_peak_columns(y, df_one, plot=False, show=False, outdir=".")
    # y_cols.shape = (L, 1)ï¼Œå–ä¸€åˆ—
    seg = y_cols[:, 0]
    # å»æ‰å¯èƒ½çš„ NaNï¼ˆç†è®ºä¸Šå·²æ‹‰ä¼¸ï¼Œæ—  NaNï¼›æ­¤å¤„ä¿é™©å¤„ç†ï¼‰
    return seg[~np.isnan(seg)]


def process_two_input_dirs(dir_a: str, dir_b: str,
                           start_block: int = 0, end_block: int = 550, max_rows: Optional[int] = None) -> Path:
    """
    ä»ä¸¤ä¸ªä¸åŒç›®å½•é‡Œï¼Œå„è‡ªæŒ‘å‡ºâ€œprominence æœ€å¤§â€çš„å³°æ®µï¼Œç„¶åè¿›è¡Œç›¸ä¼¼åº¦è¯„ä¼°ã€‚
    äº§ç‰©ç»Ÿä¸€è½åœ°åˆ°ï¼š
      ${OUT_ROOT}/_out/double/<fileA>_VS_<fileB>/
    """
    dir_a = Path(dir_a).resolve()
    dir_b = Path(dir_b).resolve()

    # 1) å…ˆæŠŠä¸¤ä¸ªç›®å½•çš„ bin -> ndarray
    bin_results_a = convert_bin_dir(
        input_dir=str(dir_a),
        npy_output_dir=None,
        start_block=start_block,
        end_block=end_block,
        make_images=False,
        return_arrays=True,
        # æ–¹å·®å›¾ä¹Ÿä¸€èµ·ä¸¢åˆ° pair ç›®å½•é‡Œ
        category_out_root=None,
    )

    bin_results_b = convert_bin_dir(
        input_dir=str(dir_b),
        npy_output_dir=None,
        start_block=start_block,
        end_block=end_block,
        make_images=False,
        return_arrays=True,
        category_out_root=None,
    )

    def _pick_first_valid(recs):
        for r in recs:
            if "error" in r:
                continue
            arr = r.get("selected")
            if arr is None or arr.size == 0:
                continue
            y = arr.astype(np.float32)
            if y.ndim == 2:
                y = y[5, :]
            return r.get("base", "unknown"), y.ravel()
        raise RuntimeError("No valid ndarray in directory")

    base_a, y_a = _pick_first_valid(bin_results_a)
    base_b, y_b = _pick_first_valid(bin_results_b)

    if max_rows is not None and max_rows > 0:
        if y_a.shape[0] > max_rows:
            y_a = y_a[:max_rows]
        if y_b.shape[0] > max_rows:
            y_b = y_b[:max_rows]

    # 2) å„è‡ªåšå³°æ£€æµ‹å¹¶å–â€œprominence æœ€å¤§â€çš„é‚£ä¸€ä¸ªå³°æ®µ
    df_peaks_a = detect_peaks_1d(
        y_a, prominence=PROMINENCE, distance=MIN_DISTANCE,
        smooth_window=SMOOTH_WINDOW, smooth_poly=SMOOTH_POLY
    )
    df_peaks_b = detect_peaks_1d(
        y_b, prominence=PROMINENCE, distance=MIN_DISTANCE,
        smooth_window=SMOOTH_WINDOW, smooth_poly=SMOOTH_POLY
    )
    if df_peaks_a.empty or df_peaks_b.empty:
        raise RuntimeError("No peaks in A or B")

    def _top_prom_segment(y, df_peaks, base_name: str, outdir_for_debug: Path, tag: str):
        # ===å»ºç«‹ä¿å­˜æ•°æ®çš„åœ°å€===
        top_dir = outdir_for_debug / f"top_{tag}"
        _ensure_dir(top_dir)  # âœ… å…³é”®ï¼šç¡®ä¿ç›®å½•å­˜åœ¨

        df1 = (
            df_peaks
            .sort_values("prominence", ascending=False)
            .head(1)
            .reset_index(drop=True)  # âœ… æŠŠè¿™ä¸€è¡ŒåŠ ä¸Š
        )

        y_peaks, x_peaks, idx_df = extract_peak_columns(
            y, df1,
            plot=True,
            base_name=base_name,
            show=False,
            outdir=str(outdir_for_debug / f"top_{tag}")
        )

        # np.save(str(outdir_for_debug / f"{base_name}_y_peaks.npy"), y_peaks)
        # np.save(str(outdir_for_debug / f"{base_name}_x_peaks.npy"), x_peaks)
        idx_df.to_csv(outdir_for_debug / f"{base_name}_peaks_idx.csv",
                      index=False, encoding="utf-8-sig")

        seg = y_peaks[:, 0]
        return seg[~np.isnan(seg)], idx_df

    # 3) æ ¹æ®æ–‡ä»¶åç¡®å®š double çš„è¾“å‡ºç›®å½•ï¼š
    pair_base = f"{base_a}_VS_{base_b}"
    outdir = _double_file_outdir(pair_base)      # => ${OUT_ROOT}/_out/double/fileA_VS_fileB/
    peaks_dir = outdir / "peaks"
    spec_dir  = outdir / "spectrum"
    rp_dir    = outdir / "rp"
    _ensure_dir(peaks_dir)
    _ensure_dir(spec_dir)
    # _ensure_dir(rp_dir)

    # è¿™é‡ŒæŠŠ peaks_dir ä¼ è¿›å»ï¼ŒåŒæ—¶ base_name åˆ†åˆ«ç”¨ base_a / base_bï¼Œæ–¹ä¾¿åŒºåˆ†
    seg_a, idx_a = _top_prom_segment(y_a, df_peaks_a, base_a, peaks_dir, "A")
    seg_b, idx_b = _top_prom_segment(y_b, df_peaks_b, base_b, peaks_dir, "B")
    L = min(len(seg_a), len(seg_b))
    if L < 2:
        raise ValueError(f"ä¸¤è·¯æœ€å¤§å³°é•¿åº¦å¤ªçŸ­: len(A)={len(seg_a)}, len(B)={len(seg_b)}")
    seg_a = seg_a[:L]
    seg_b = seg_b[:L]

    # å¯é€‰ï¼šæŠŠè¿™ä¿©æ®µä¿å­˜ä¸‹æ¥
    # np.save(str(peaks_dir / f"{pair_base}_seg_a.npy"), seg_a)
    # np.save(str(peaks_dir / f"{pair_base}_seg_b.npy"), seg_b)

    # 4) é¢‘è°± + è°æ³¢
    freq_a, amp_a = safe_plot_spectrum(
        seg_a, FS_SIGNAL,
        title=f"{pair_base} - A_topProm",
        out_path=str(spec_dir / f"{pair_base}_A_topProm.png")
    )
    hfa, haa = find_harmonics(freq_a, amp_a, height=PEAK_HEIGHT)
    pd.DataFrame({"harmonic_freq": hfa, "harmonic_amp": haa}) \
      .to_csv(spec_dir / f"{pair_base}_A_harmonics.csv",
              index=False, encoding="utf-8-sig")

    freq_b, amp_b = safe_plot_spectrum(
        seg_b, FS_SIGNAL,
        title=f"{pair_base} - B_topProm",
        out_path=str(spec_dir / f"{pair_base}_B_topProm.png")
    )
    hfb, hab = find_harmonics(freq_b, amp_b, height=PEAK_HEIGHT)
    pd.DataFrame({"harmonic_freq": hfb, "harmonic_amp": hab}) \
      .to_csv(spec_dir / f"{pair_base}_B_harmonics.csv",
              index=False, encoding="utf-8-sig")

    # 5) RP + å›¾åƒç›¸ä¼¼åº¦
    y_peaks = np.stack([seg_a, seg_b], axis=1)  # shape = (L, 2)
    RP_images = create_RP(y_peaks, eps=0.005, steps=255, OUTDIR=str(rp_dir))
    rp_final, rp_details = compute_image_similarity_RP(
        RP_images[0], RP_images[1], return_details=True
    )
    rp_ssim = rp_details["ssim"]
    rp_ncc  = rp_details["ncc"]
    rp_hist = rp_details["hist"]

    # 6) é¢‘è°±ç›¸ä¼¼åº¦
    freq_score, parts = combined_signal_similarity(
        seg_a, seg_b,
        fs=FS_SIGNAL,
        nperseg=WELCH_NPERSEG,
        noverlap=WELCH_NOVERLAP,
        fmin=ANALYSIS_BAND[0],
        fmax=ANALYSIS_BAND[1],
    )

    # 7) äº‹ä»¶çº§ POCï¼ˆæ•´æ®µè§†ä½œä¸€ä¸ªäº‹ä»¶ï¼‰
    events = [(0, L)]

    poc_df = batch_events_similarity(seg_a, seg_b, events, POC_PARAMS)
    poc_df["freq_score"]       = freq_score
    poc_df["cos_psd_simple"]   = parts["cos_psd"]
    poc_df["coherence_simple"] = parts["coherence"]
    poc_df["xcorr_simple"]     = parts["xcorr"]

    sim_dir = outdir / "similarity"
    _ensure_dir(sim_dir)
    poc_df.to_csv(
        sim_dir / f"{pair_base}_similarity_detail.csv",
        index=False, encoding="utf-8-sig"
    )

    # 8) æ‰“åŒ…æˆä¸€è¡Œæ±‡æ€»
    poc_row = poc_df.iloc[0] if not poc_df.empty else None
    file_df = pack_file_result(
        base=pair_base,
        rp_final=rp_final, rp_ssim=rp_ssim, rp_ncc=rp_ncc, rp_hist=rp_hist,
        freq_score=freq_score, parts=parts,
        poc_df_row=poc_row,
    )

    # å†™ä¸€ä¸ª pair çº§åˆ«çš„æ±‡æ€» CSV åˆ°åŒä¸€ç›®å½•
    summary_path = outdir / "all_similarity_summary.csv"
    if summary_path.exists():
        old = pd.read_csv(summary_path)
        new = pd.concat([old, file_df], ignore_index=True)
    else:
        new = file_df
    new.to_csv(summary_path, index=False, encoding="utf-8-sig")

    return outdir


# ----------------------
# ç›®å½•çº§å¤„ç†ï¼ˆä¾› watcher/runner è°ƒç”¨ï¼‰
# ----------------------
def _filter_results_by_files(recs: List[dict], only_files: Optional[Iterable[str]]) -> List[dict]:
    if not only_files:
        return recs
    allow: Set[str] = {Path(f).stem for f in only_files}
    return [r for r in recs if r.get("base") in allow]

def process_hour_dir(
    hour_dir: Path,
    start_block: int = 0,
    end_block: int = 550,
    max_rows: Optional[int] = None,
) -> Path:
    """å¤„ç†ä¸€ä¸ªå°æ—¶ç›®å½•ï¼ŒæŠŠäº§ç‰©å†™åˆ° <hour_dir>/_out/ å¹¶ç”Ÿæˆç›®å½•æ±‡æ€» CSVã€‚"""
    hour_dir = Path(hour_dir)
    out_root = _new_run_outdir(hour_dir, "single_file_results")

    bin_results = convert_bin_dir(
        input_dir=str(hour_dir),
        npy_output_dir=None,
        start_block=start_block,
        end_block=end_block,
        make_images=True,
        return_arrays=True,
        category_out_root=str(out_root),
    )

    rows: List[pd.DataFrame] = []
    for rec in tqdm(bin_results, desc=f"Process chain in {hour_dir.name}"):
        base = rec.get("base", "unknown")
        if "error" in rec:
            rows.append(pd.DataFrame([{"file": base, "error": rec["error"]}]))
            continue
        arr = rec.get("selected")
        if arr is None:
            rows.append(pd.DataFrame([{"file": base, "error": "missing selected ndarray"}]))
            continue
        try:
            file_df = process_single_array(
                arr,
                path=str(hour_dir / f"{base}.bin"),
                max_rows=max_rows,               # â­ ä¼ è¿›å»
            )
        except Exception as e:
            file_df = pd.DataFrame([{"file": base, "error": str(e)}])
        rows.append(file_df)

    if rows:
        summary = pd.concat(rows, ignore_index=True)
        (out_root / "all_similarity_summary.csv").write_text(
            summary.to_csv(index=False, encoding="utf-8-sig"),
            encoding="utf-8-sig"
        )
    return out_root

def process_selected_files(
        hour_dir: Path,
        files: List[str],
        start_block: int = 0,
        end_block: int = 550,
        max_rows: Optional[int] = None,
        ) -> Path:
    """åªå¤„ç†ä¼ å…¥æ–‡ä»¶ï¼ˆæŒ‰ stem è¿‡æ»¤ï¼‰ã€‚"""
    max_rows = max_rows
    hour_dir = Path(hour_dir)
    out_root = _new_run_outdir(hour_dir, "single_file_results")

    bin_results = convert_bin_dir(
        input_dir=str(hour_dir),
        npy_output_dir=None,
        start_block=start_block,
        end_block=end_block,
        make_images=True,
        return_arrays=True,
        image_dir=str(out_root),
    )

    bin_results = _filter_results_by_files(bin_results, files)

    rows: List[pd.DataFrame] = []
    for rec in tqdm(bin_results, desc=f"Process selected in {hour_dir.name}"):
        base = rec.get("base", "unknown")
        if "error" in rec:
            rows.append(pd.DataFrame([{"file": base, "error": rec["error"]}]))
            continue
        arr = rec.get("selected")
        if arr is None:
            rows.append(pd.DataFrame([{"file": base, "error": "missing selected ndarray"}]))
            continue
        try:
            file_df = process_single_array(arr, path=str(hour_dir / f"{base}.bin"), max_rows=max_rows)
        except Exception as e:
            file_df = pd.DataFrame([{"file": base, "error": str(e)}])
        rows.append(file_df)

    if rows:
        summary = pd.concat(rows, ignore_index=True)
        (out_root / "all_similarity_summary.csv").write_text(
            summary.to_csv(index=False, encoding="utf-8-sig"),
            encoding="utf-8-sig"
        )
    return out_root

# ======================
# ä¸»æµç¨‹ï¼šéå†ç›®å½•ï¼ˆé€’å½’ï¼‰ï¼Œå¹¶æŒ‰â€œæ¯ä¸ªç›®å½•â€å„è‡ªè½åœ°æ±‡æ€»è¡¨
# ======================
def main():
    """
    ä»…å¤„ç† .binï¼š
    - åœ¨ LIST_DIR ä¸‹é€’å½’æŸ¥æ‰¾å„ç›®å½•çš„ .bin
    - convert_bin_dir è´Ÿè´£è¯»å–/åˆ‡ç‰‡ï¼ˆå†…éƒ¨è‡ªå¸¦ tqdmï¼Œæ˜¾ç¤ºâ€œBINâ†’NPY/Arrayâ€ï¼‰
    - å¯¹æ¯ä¸ªè¿”å›çš„è®°å½•å†ç”¨ tqdm åšåç»­å¤„ç†ï¼ˆprocess_single_arrayï¼‰
    - äº§ç‰©è½åˆ° <dir>/_out/<base>/...ï¼›ç›®å½•çº§æ±‡æ€» <dir>/_out/all_similarity_summary.csv
    """
    if not os.path.isdir(LIST_DIR):
        print(f"[ERROR] LIST_DIR not found: {LIST_DIR}")
        return

    any_bin = False
    dir_summaries: Dict[str, List[pd.DataFrame]] = {}

    for dirpath, _, files in os.walk(LIST_DIR):
        # æ‰¾åˆ°è¯¥ç›®å½•ä¸‹çš„ .bin
        bin_files = sorted([f for f in files if f.lower().endswith(".bin")])
        if not bin_files:
            continue

        any_bin = True
        local_rows: List[pd.DataFrame] = []

        # ç›®å½•çº§è¾“å‡ºï¼šæŠŠæ–¹å·®å›¾/variance.npy æ”¾åœ¨ <dir>/_out/
        dir_out = _dir_outdir(dirpath, OUTPUT_SUBDIR_NAME)

        # åªç”¨å†…å­˜æ•°ç»„ï¼Œä¸”ï¼ˆæŒ‰éœ€ï¼‰ç”»æ–¹å·®å›¾ï¼›ä¸è½ .npy
        bin_results = convert_bin_dir(
            input_dir=dirpath,
            npy_output_dir=None,                # ä¸ä¿å­˜ .npy
            start_block=0,
            end_block=550,
            make_images=True,                   # æ–¹å·®å›¾/variance.npy â†’ <dir>/_out/
            return_arrays=True,                 # è¿”å› selected ndarray
            image_dir=dir_out
        )

        # å¯¹æ¯ä¸ª bin ç»“æœåšåç»­å¤„ç†ï¼ˆè¿™é‡Œå†ç”¨ tqdm å±•ç¤ºâ€œå¤„ç†é“¾â€çš„è¿›åº¦ï¼‰
        for rec in tqdm(bin_results, desc=f"Process chain in {os.path.basename(dirpath) or dirpath}"):
            base = rec.get("base", "unknown")

            # è‹¥ convert_bin_dir æŠŠé”™è¯¯æ”¾è¿› rec["error"]ï¼Œè¿™é‡Œç›´æ¥è·³è¿‡å¹¶è®°å½•
            if "error" in rec:
                print(f"[SKIP][BIN] {base}: {rec['error']}")
                local_rows.append(pd.DataFrame([{"file": base, "error": rec["error"]}]))
                continue

            try:
                arr = rec.get("selected", None)
                if arr is None:
                    # å¯ä»¥å›é€€ç”¨ out_npyï¼›è¿™é‡Œåªä¸“æ³¨â€œåªç”¨å†…å­˜â€çš„éœ€æ±‚ï¼Œç›´æ¥è·³è¿‡
                    print(f"[SKIP][BIN] {base}: missing 'selected' ndarray in result")
                    local_rows.append(pd.DataFrame([{"file": base, "error": "missing selected ndarray"}]))
                    continue

                # è¿›å…¥å¤„ç†é“¾ï¼›æ³¨æ„å‚æ•°åè¦ä¸å‡½æ•°å®šä¹‰ä¸€è‡´
                # å¦‚æœéœ€è¦â€œæ–¹å·®æœ€å¤§è¡Œ/å‡å€¼â€ç­–ç•¥ï¼Œå¯ä¼  trace_strategy="maxvar"/"mean"
                bin_path = os.path.join(dirpath, f"{base}.bin")
                file_df = process_single_array(arr, path=bin_path)

            except Exception as e:
                print(f"[SKIP][BIN] {base}: error -> {e}")
                file_df = pd.DataFrame([{"file": base, "error": str(e)}])

            local_rows.append(file_df)

        # ç›®å½•çº§æ±‡æ€»
        if local_rows:
            summary = pd.concat(local_rows, ignore_index=True)
            save_path = os.path.join(dir_out, "all_similarity_summary.csv")
            summary.to_csv(save_path, index=False, encoding="utf-8-sig")
            print(f"âœ… ç›®å½•çº§æ±‡æ€»å·²ä¿å­˜: {save_path}")

        dir_summaries[dirpath] = local_rows

    if not any_bin:
        print(f"[WARN] No .bin found under {LIST_DIR}")
        return

    # ï¼ˆå¯é€‰ï¼‰å…¨å±€æ±‡æ€»
    global_rows = [df for rows in dir_summaries.values() for df in rows if rows]
    if global_rows:
        global_summary = pd.concat(global_rows, ignore_index=True)
        global_out = os.path.join(os.path.abspath(LIST_DIR), "_global_out")
        os.makedirs(global_out, exist_ok=True)
        global_csv = os.path.join(global_out, "all_similarity_summary.csv")
        global_summary.to_csv(global_csv, index=False, encoding="utf-8-sig")
        print(f"ğŸŒ å…¨å±€æ±‡æ€»å·²ä¿å­˜: {global_csv}")

if __name__ == "__main__":
    dir_a = r'../data/IQ/double/IQ/40/2025-11-12 11/'
    dir_b = r'../data/IQ/double/IQ/45/2025-11-12 11/'
    process_two_input_dirs(dir_a, dir_b,
                               start_block= 0, end_block = 768)
